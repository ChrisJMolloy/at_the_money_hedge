import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

import wrds
WRDS_LOGIN = '' # replace login on coursework

db = wrds.Connection(wrds_username=WRDS_LOGIN)





print(f'List of all tables in the wrdsapps_link_crsp_optionm library: {db.list_tables(library='wrdsapps_link_crsp_optionm')}')


query = """
        SELECT *
        FROM wrdsapps_link_crsp_optionm.opcrsphist
        LIMIT 5
        """
db.raw_sql(query, date_cols=['sdate', 'exdedateate'])





# provided in problem
google_permco = 45483

query = f"""
        SELECT DISTINCT wrdsapps_link_crsp_optionm.opcrsphist.secid,
        crsp.stocknames_v2.permno,
        wrdsapps_link_crsp_optionm.opcrsphist.sdate,
        wrdsapps_link_crsp_optionm.opcrsphist.edate
        FROM crsp.stocknames_v2
        JOIN
        wrdsapps_link_crsp_optionm.opcrsphist
        ON crsp.stocknames_v2.permno = wrdsapps_link_crsp_optionm.opcrsphist.permno
        WHERE crsp.stocknames_v2.permco = {google_permco}
        """
google_option_data_df = db.raw_sql(query)
google_option_data_df





# will be helpful later
class_a = 121812
class_c = 203876





sec_ids = '121812, 203876'


%%time
year = 2022
query= f"""
        SELECT *
        FROM optionm.opprcd{year}
        WHERE secid in ({sec_ids})
        """
dl_2022 = db.raw_sql(query, date_cols=['date', 'exdate'])


%%time
year = 2023
query= f"""
        SELECT *
        FROM optionm.opprcd{year}
        WHERE secid in ({sec_ids})
        """
dl_2023 = db.raw_sql(query, date_cols=['date', 'exdate'])


g_options = pd.concat([dl_2022, dl_2023])
g_options = g_options.reset_index(drop=True)


query = f"""
        SELECT * 
        FROM optionm.secprd
        WHERE secid in ({sec_ids})
        AND date >= '2021-12-31'
        """
GOOG = db.raw_sql(query, date_cols=['date'])


GOOG.tail()





g_options.tail()





# most recent adjustment factor
last_adj = GOOG.reset_index().sort_values(by='date').groupby('secid')['cfadj'].last().to_frame()
last_adj = last_adj.rename(columns={'cfadj':'last_adj'})
GOOG = GOOG.set_index('secid').join(last_adj).reset_index()
# get google adjusted closing price
GOOG['adj_close'] = GOOG['close'] * GOOG['cfadj'] / GOOG['last_adj']


GOOG.pivot(index='date', columns='secid', values='close').plot(logy=True, title='Price of Google with Log Scale', ylabel='Price', xlabel='Day');
GOOG.pivot(index='date', columns='secid', values='adj_close').plot(title='Price of Google (Adjusted for Corporate Action)', ylabel='Price', xlabel='Day');





# adjust option bid, ask, and strike to match
option_last_adj = g_options.sort_values(by='date').groupby('optionid')['cfadj'].last().to_frame()
option_last_adj = option_last_adj.rename(columns={'cfadj':'last_adj'})
g_options = g_options.set_index('optionid').join(option_last_adj).reset_index()
g_options['adj_strike_price'] = g_options['strike_price'] * g_options['cfadj'] / g_options['last_adj']
g_options['adj_bid'] = g_options['best_bid'] * g_options['cfadj'] / g_options['last_adj']
g_options['adj_offer'] = g_options['best_offer'] * g_options['cfadj'] / g_options['last_adj']





GOOG = GOOG.set_index(['date', 'secid'])
g_options = g_options.set_index(['date', 'secid'])


g_options = g_options.merge(GOOG['adj_close'], how='left', right_index=True, left_on=['date', 'secid'])
g_options = g_options.reset_index()





# code used in lecture to produce "B-S like columns"

g_options['K'] = g_options['adj_strike_price'] / 1000
# ^ optionmetrics convention, nothing super interesting under the hood, ensure to observe documentation
g_options['V0'] = (g_options['adj_bid'] + g_options['adj_offer']) / 2 # rough true price
g_options = g_options.rename(columns={'adj_close': 'S0'}) # B&S S0 price when pricing option
# < 1: out of the money
# = 1: on the money
# > 1: in the money
g_options['M0'] = g_options['S0'] / g_options['K']  # "moneyness"
# time left to T in years
g_options['tau'] = (g_options['exdate'] - g_options['date']).dt.days / 360


num_options = len(g_options['optionid'].unique())
print(f'Number of options before filtering: {round(num_options, 3)}')


# filtering code very similar to what is used in lecture notes


# filter by missing price data
bl = g_options['S0'].isnull() | g_options['V0'].isnull()
g_options = g_options[~bl]

# filter by missing delta
bl = g_options['delta'].isnull()
g_options = g_options[~bl]

# filter by open interest (contracts that exist)
bl = g_options['open_interest'].eq(0)
g_options = g_options[~bl]

# filter by volume
bl = g_options['volume'].eq(0)
g_options = g_options[~bl]

# filter out options with negative time value
# This is related to arbitrage
# here we assume r = 0
bl_c = (g_options['cp_flag'] == 'C') & (g_options['S0'] - g_options['K'] >= g_options['V0'])
bl_p = (g_options['cp_flag'] == 'P') & (g_options['K'] - g_options['S0'] >= g_options['V0'])
bl = bl_c | bl_p
g_options = g_options[~bl]

# filter out not at-the-money options
bl = ((g_options['M0'] > 1.01) | (g_options['M0'] < 0.99))
g_options = g_options[~bl]

# strictly between: (21, 35)
# filter out by maturity pt 1
# remove less than or equal to 21 days
bl = g_options['tau'].le(21/360)
g_options = g_options[~bl]

# filter out by maturity pt 2
# remove greater than or equal to 35 days
bl = g_options['tau'].ge(35/360)
g_options = g_options[~bl]


num_options = len(g_options['optionid'].unique())
print(f'Number of options after filtering: {num_options}')





# group by security id, then group by the date, then find the mean of this
imp_vol_class_df = g_options.groupby(['secid', 'date'])['impl_volatility'].mean().to_frame().reset_index()


#pivot and clean up names
imp_vol_class_df = imp_vol_class_df.pivot(index='date', columns='secid', values='impl_volatility')
imp_vol_class_df = imp_vol_class_df.rename(columns={class_a:'Class A', class_c:'Class C'})
imp_vol_class_df.columns = imp_vol_class_df.columns.rename('Share Type')
imp_vol_class_df = imp_vol_class_df * 100 # percent


# make sure we cover the entire trading period
imp_vol_class_df[pd.isnull(imp_vol_class_df).any(axis=1)].head()





imp_vol_class_df = imp_vol_class_df.ffill()


imp_vol_class_df.plot(title='Implied Volatility of Google Shares: Class A vs. C', ylabel='Implied Volatility (%)', xlabel='Day');





(imp_vol_class_df['Class A'] - imp_vol_class_df['Class C']).plot(title='Difference between Class A and Class C', ylabel='Difference of Percent', xlabel='Day');





# group by option type, then group by the date, then find the mean of this
imp_vol_opt_df = g_options.groupby(['cp_flag', 'date'])['impl_volatility'].mean().to_frame().reset_index()
#pivot and clean up names
imp_vol_opt_df = imp_vol_opt_df.pivot(index='date', columns='cp_flag', values='impl_volatility')
imp_vol_opt_df = imp_vol_opt_df.rename(columns={'C':'Call', 'P':'Put'})
imp_vol_opt_df.columns = imp_vol_opt_df.columns.rename('Option Type')
imp_vol_opt_df = imp_vol_opt_df * 100 # percent


imp_vol_opt_df.plot(title='Implied Volatility of Google Shares: Calls vs. Puts', ylabel='Implied Volatility (%)', xlabel='Day');





(imp_vol_opt_df['Call'] - imp_vol_opt_df['Put']).plot(title='Difference between Call and Put', ylabel='Difference of Percent');





interest_opt_df = g_options.groupby(['cp_flag', 'date'])['open_interest'].mean().to_frame().reset_index()
#pivot and clean up names
interest_opt_df = interest_opt_df.pivot(index='date', columns='cp_flag', values='open_interest')
interest_opt_df = interest_opt_df.rename(columns={'C':'Call', 'P':'Put'})
interest_opt_df.columns = interest_opt_df.columns.rename('Option Type')

interest_opt_df.plot(title='Option Open Interest', xlabel='Day', ylabel='Options Outstanding');








g_options.loc[g_options['optionid'] == 135993816][['date', 'optionid', 'best_bid']]


g_options.loc[(g_options['date'].between('2022-12-21', '2022-12-27')) & (g_options['optionid'] == 135993869)][['date', 'optionid', 'best_bid']]





trading_days = g_options['date'].unique()

# The following deepseek promt was used to aid in the generation of this code:

#I have a pandas dataframe with trading days, option ids, and the price, I want
#to find all options that do not have consecutive trading days MAKE SURE IT CONSIDERS TRADING DAYS

def get_missing_days(group):
    option_dates = group['date'].unique()
    min_date, max_date = option_dates.min(), option_dates.max()
    expected_days = trading_days[(trading_days >= min_date) & (trading_days <= max_date)]
    missing = set(expected_days) - set(option_dates)
    if len(missing) < 1:
        missing = np.nan
    return pd.Series({'missing_days': missing})


# remove non-consecutive traded options
gap_details = g_options.groupby('optionid').apply(get_missing_days, include_groups=False)
gap_details = gap_details.isna()
gap_details = gap_details.rename(columns={'missing_days':'consecutive'})
g_options = g_options.merge(gap_details, on='optionid')
g_options = g_options.loc[g_options['consecutive'] == True]


# sort by time
g_options = g_options.sort_values('date')
# group by contract to get contract specific information
grouped = g_options.groupby('optionid')

# now that all of our options are consecutive, we do not need to worry about look ahead bias here!
g_options['S1'] = grouped['S0'].shift(-1) # doing this makes sure we done overlap between options, grouped must keep index
g_options['V1'] = grouped['V0'].shift(-1)


# filter by missing price data
bl = g_options['S1'].isnull() | g_options['V1'].isnull()
g_options = g_options[~bl]


num_options = len(g_options['optionid'].unique())
print(f'Number of options after filtering: {num_options}')





# deepseek was used to generate the following code block using the following prompt:

# "i have a pandas dataframe of multiple options with their deltas for each day,
# I want to adda column which is "5 day delta" which is a rolling block of the same delta for 5 days"


# Sort by option_id and date to ensure correct ordering
g_options = g_options.sort_values(['optionid', 'date'])

# Create a 5-day block indicator within each option group
g_options['5_day_block'] = g_options.groupby('optionid').cumcount() // 5

# Get the first delta value for each 2-day block within each option
g_options['5_day_delta'] = g_options.groupby(['optionid', '5_day_block'])['delta'].transform('first')

# Clean up (remove temporary column if needed)
g_options = g_options.drop(columns=['5_day_block'])



# do the same thing for 2-day block

# Sort by option_id and date to ensure correct ordering
g_options = g_options.sort_values(['optionid', 'date'])

# Create a 2-day block indicator within each option group
g_options['2_day_block'] = g_options.groupby('optionid').cumcount() // 2

# Get the first delta value for each 2-day block within each option
g_options['2_day_delta'] = g_options.groupby(['optionid', '2_day_block'])['delta'].transform('first')

# Clean up (remove temporary column if needed)
g_options = g_options.drop(columns=['2_day_block'])


# normalizing everything in terms of S0
# very similar code used in lecture notes
for feature in ['S0', 'S1', 'V0', 'V1']:
    g_options[feature+'_n'] = g_options[feature] / g_options['S0'] * 100


def pnl(df, delta):
    # end cash given that we short an option
    pnl = delta*(df['S1_n'] - df['S0_n']) - (df['V1_n'] - df['V0_n'])
    return pnl


# mse computation is from lecture notes

pnl_no_hedge_1_day = pnl(g_options, 0)
pnl_BS_1_day = pnl(g_options, g_options['delta'])

# compute MSE
mse_no_hedge_1_day = (pnl_no_hedge_1_day** 2).mean()
mse_BS_1_day = (pnl_BS_1_day** 2).mean()

reduction_1_day = 100 * (1-mse_BS_1_day/mse_no_hedge_1_day)


pnl_no_hedge_2_day = pnl(g_options, 0)
pnl_BS_2_day = pnl(g_options, g_options['2_day_delta'])

# compute MSE
mse_no_hedge_2_day = (pnl_no_hedge_2_day** 2).mean()
mse_BS_2_day = (pnl_BS_2_day** 2).mean()

reduction_2_day = 100 * (1-mse_BS_2_day/mse_no_hedge_2_day)


pnl_no_hedge_5_day = pnl(g_options, 0)
pnl_BS_5_day = pnl(g_options, g_options['5_day_delta'])

# compute MSE
mse_no_hedge_5_day = (pnl_no_hedge_5_day** 2).mean()
mse_BS_5_day = (pnl_BS_5_day** 2).mean()

reduction_5_day = 100 * (1-mse_BS_5_day/mse_no_hedge_5_day)


res = {
    'Hedging Frequency': [1,2,5],
    'M-S PnL Hedgeless':[mse_no_hedge_1_day,mse_no_hedge_2_day,mse_no_hedge_5_day],
    'M-S PnL Hedged': [mse_BS_1_day, mse_BS_2_day, mse_BS_5_day],
    'M-S PnL Reduction (%)': [reduction_1_day, reduction_2_day, reduction_5_day]
      }
pd.DataFrame(res).set_index('Hedging Frequency')





# overlapping histogram builds on lecture notes

ax= pnl_no_hedge_1_day.hist(bins=10, label='No Hedging', density=True)
pnl_BS_1_day.hist(bins=10, label='BS Hedging (Daily)', density=True)
pnl_BS_2_day.hist(bins=10, label='BS Hedging (2-Day)', density=True)
pnl_BS_5_day.hist(bins=10, label='BS Hedging (5-Day)', density=True)
ax.legend(frameon=False)
ax.set_xlabel('PnL ($)')
ax.set_ylabel('Density')
ax.set_title('Daily PnL of Portfolios');





db.close()
